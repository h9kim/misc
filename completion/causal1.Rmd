Matrix completion as semi-supervised regression
========================================================

Prediction using glmnet and softImpute
```{r}
library(magrittr, quietly=TRUE, warn.conflicts=FALSE)
library(pracma, quietly=TRUE, warn.conflicts=FALSE)
library(Matrix, quietly=TRUE, warn.conflicts=FALSE, verbose=FALSE)
library(foreach, quietly=TRUE, warn.conflicts=FALSE, verbose=FALSE)
library(glmnet, quietly=TRUE, warn.conflicts=FALSE, verbose=FALSE)
library(softImpute, quietly=TRUE, warn.conflicts=FALSE)
```

ALS custom written for potential outcomes
```{r}
als <- function(obs, r, lx = 0, ly = lx, nits = 10) {
  n <- dim(obs)[1]; p <- dim(obs)[2]
  treat <- ifelse(is.na(obs[, 1]), 1, 0)
  obs2 <- obs
  obs2[is.na(obs2)] <- mean(obs[, 1:2], na.rm=TRUE)
  res0 <- svd(obs2)
  x <- res0$u[, 1:r, drop = FALSE]
  y <- res0$v[, 1:r, drop = FALSE]
  for (it in 1:nits) {
    # update x
    regx <- y[-2, , drop = FALSE]
    regy <- t(obs[treat == 0, -2])
    x[treat==0, ] <- solve(t(regx) %*% regx + ly * eye(r), t(regx) %*% regy)
    regx <- y[-1, , drop = FALSE]
    regy <- t(obs[treat == 1, -1])
    x[treat==1, ] <- solve(t(regx) %*% regx + ly * eye(r), t(regx) %*% regy)
    
    # update y
    for (i in 1:p) {
      filt <- !is.na(dat[, i])
      regmat <- x[filt, , drop = FALSE]
      y[i, ] <- 
        solve(t(regmat) %*% regmat + lx * eye(r), t(regmat) %*% dat[filt, i])    
    }  
  }
  comp <- x %*% t(y)
  comp
}
```

# Potential outcomes model

The model is as follows.  $x_i$ are covariates of the $i$th subject, $d_i$ is a binary variable indicating treatment or control.  This is observational data, so $d_i$ depends on $x_i$.  We model
\[
\text{Pr}[D_i = 1] = \frac{e^{x_i^T \beta}}{1 + e^{x_i^T \beta}}
\]

Generate a low-rank matrix $(Y^0 Y^1 X)$.  Then determine $D$, the treatment assignment, as a function of $X$, by the above equation.  $Y^0$ is observed for all cases where $D = 0$ and $Y^1$ is observed for all cases $D = 1$.

```{r}
n <- 10000
p <- 20
k <- 3
sigma <- 1e-3
utrue <- randn(n, k)
vtrue <- randn(p, k)
# treatment effect
true_effect <- rnorm(1)
utrue[, k] <- 1
vtrue[, k] <- true_effect * c(0, 1, rep(0, p-2))
full<- utrue %*% t(vtrue) + sigma * randn(n, p)
colnames(full) <- c("y0", "y1", paste0("X", 1:(p-2)))
bt <- rnorm(p - 2) # variable determining treatment
temp <- full[, -(1:2)] %*% bt
# increase variation in treatment probabilities
temp <- 3 * temp/sd(temp)
probs <- temp  %>% {exp(.) / (1 + exp(.))}
treat <- rbinom(n=n, size=1, prob=probs)
obs <- full
obs[treat == 1, 1] <- NA
obs[treat == 0, 2] <- NA
y0 <- full[, 1]
y1 <- full[, 2]
X <- cbind(treat, full[, -c(1:2)])
y_obs <- ifelse(treat == 0, y0, y1)
```

Unbiased estimate of treatment effect $\E[Y^1 - Y^0]$
```{r}
(unbiased_effect <- mean(y1) - mean(y0))
```

Naive regression
```{r}
res <- lm(y_obs ~ X)
(naive_effect <- coef(res)[2])
```

Alternating regression
```{r}
cases0 <- cbind(y_obs, X[, -1])[treat == 0, ]
cases1 <- cbind(y_obs, X[, -1])[treat == 1, ]
res0 <- lm(cases0[, 1] ~ cases0[, -1])
res1 <- lm(cases1[, 1] ~ cases1[, -1])
yh0 <- coef(res0) %>% {.[1] + cases1[, -1] %*% .[-1]}
yh1 <- coef(res1) %>% {.[1] + cases0[, -1] %*% .[-1]}
(alt_effect <- mean(c(yh1, cases1[, 1]) - c(cases0[, 1], yh0)))
```

Matrix completion
```{r}
fit <- softImpute(obs, rank.max=k, type="als", lambda = 1e-3)
pre <- complete(obs, fit)
(mc_effect <- mean(pre[, 2] - pre[, 1]))
```

ALS
```{r}
pre_als <- als(obs, r=k, lx=1e-3)
(als_effect <- mean(pre_als[, 2] - pre_als[, 1]))
```

ALS with truth
```{r}
pre_als <- als(obs, r=k, lx=1e-3, x=utrue, y=vtrue)
(als_effect <- mean(pre_als[, 2] - pre_als[, 1]))
```
