%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\newcommand{\xmark}{\textcolor{red}{\text{\sffamily X}}}
\newcommand{\cmark}{\textcolor{green}{\checkmark}}
\newcommand{\tr}{\text{tr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Vol}{\text{Vol}}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------


\title{A practical evaluation of recent methods in high-dimensional inference}

\author{Charles Zheng} % Your name
\institute[Stanford] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{Stanford University}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\section{Introduction}

\begin{frame}
\frametitle{Problem and motivation}
\begin{itemize}
\item $x \in \mathbb{R}^p, y \in \mathbb{R}$ have a joint distribution $P$ where $y|x \sim N(x^T \beta, \sigma^2)$
\item Observe $X = (x_1, \hdots, x_n)^T$, $Y = (y_1,\hdots, y_n)$ iid
\item Problem: test $H_i: \beta_0 = i$ for $i = 1,\hdots, p$
\item Motivation: $x$ are SNPs (mutations), $y$ is phenotype
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Methods}
\begin{center}
\begin{tabular}{|c|c|c|} \hline
 & Control & $p > n$\\ \hline
Classical inference (Pearson 1930) & Marginal & No \\ \hline
Covariance test (Lockhart et al. 2014) &  FWER? & Yes \\ \hline
Debiased lasso (Javanmard et al. 2014) & Marginal & Yes\\ \hline
Knockoffs (Barber et al. 2014) & FDR & ? \\ \hline
 & &  \\ \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{The LASSO path}
\[
\hat{\beta}_\lambda = \text{argmin}_\beta \frac{1}{2}||X\beta - Y||^2 + \lambda ||\beta||_1
\]
\begin{center}
\includegraphics[scale = 0.25]{lasso_path.png}
\end{center}
\emph{(Image credit: ??)}
\end{frame}

\begin{frame}
\frametitle{Covariance test}
\begin{itemize}
\item (2014) Lockhart, Taylor, Tibshirani (x 2)
\item Standard assumptions $Y \sim N(X\beta, \sigma^2 I)$ + large $p$ asymptotics
\item \emph{See also} non-asymptotic exact test (Lee, Sun x 2, Taylor 2015)
\end{itemize}
\begin{center}
\includegraphics[scale = 0.25]{covtest.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Debiased regularized M-estimators}
\begin{itemize}
\item (2014) Javanmard and Montanari
\item Standard assumptions + sparsity condition on $\beta$ + large $n$ and $p$ asymptotics
\end{itemize}
\begin{center}
\includegraphics[scale = 0.25]{javanmard.png}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Knockoff filter}
\begin{itemize}
\item (2014) Barber and Cand\'{e}s
\item \emph{Finite sample} $Y \sim N(X\beta, \sigma^2 I)$, $n \leq p$, control FDR
\item Extension to $p > n$, FWER control, etc. forthcoming...
\end{itemize}
\begin{center}
\begin{tabular}{cc}
\includegraphics[scale = 0.25]{knockoff.png} &
\includegraphics[scale = 0.5, trim=0in -1in 0.5in 0in, clip]{knockoff2.png}
\end{tabular}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Methods}
But what's actually used in practice?
\begin{center}
\begin{tabular}{|c|c|c|} \hline
 & Control & $p > n$\\ \hline
Classical inference (Pearson 1930) & Marginal & No \\ \hline
Covariance test (Lockhart et al. 2014) &  FWER? & Yes \\ \hline
Debiased lasso (Javanmard et al. 2014) & Marginal & Yes\\ \hline
Knockoffs (Barber et al. 2014) & FDR & ? \\ \hline
\textbf{Marginal screening} & ??? & Yes \\ \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Regression vs Marginal Screening}
Testing $H_i: \beta_i = 0$ is better than testing $H_i: \Cov(X_i, Y) = 0$
when you are looking for $X_i$ \emph{directly} linked to $Y$
\begin{center}
\begin{tabular}{cc}
\multirow{5}{*}{\includegraphics[scale = 0.3]{pgm.png}} & \\
& $\beta_i = 0$\\
& \vspace{0.2in}\\
& $\beta_i \neq 0$\\
& \vspace{0.2in}\\
\end{tabular}
\end{center}
(Adapted from \emph{Mourad 2012})
\end{frame}

\begin{frame}
\frametitle{Practical Validation}
\begin{itemize}
\item These procedures are derived under strong assumptions (and slightly different model, fixed $X$), \emph{how well do they work in real data}?
\item We could validate inference procedures in real data if only we knew the `\emph{true}' $\beta$, defined as
\[
\beta = \E[x x^T]^{-1} \E[yx]
\]
\item Possibility: take a dataset with large $n$ \emph{and} large $p$ (so we can estimate $\beta$ easily using OLS) and test procedure on a subset $n_0 << n$ of the data
\item Or...
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Idea}
I give you real data \emph{mixed in} with noise variables
\begin{center}
\includegraphics[scale = 0.35]{anc.png}
\end{center}
\begin{itemize}
\item Can you identify the original columns from the noise columns?
\item I can test your procedure this way, because I know the ground truth!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Synthetic Negative Controls}
\begin{itemize}
\item<1-> Given random vector $x \in \mathbb{R}^p$, \emph{define} $\tilde{x} \in \mathbb{R}^{p+q}$ by
by
\[
\tilde{x} = \begin{pmatrix}I \\ \Gamma\end{pmatrix} x + e
\]
where $\Gamma$ is a fixed matrix and $e \perp x, y$.
\item<2-> Let
\[
\beta = \E[xx^T]^{-1}\E[yx], \ \ \tilde{\beta} = \E[\tilde{x}\tilde{x}^T]^{-1} \E[y\tilde{x}]
\]
\item<3-> Then
\[
\forall i \in \{1, \hdots, p\}: \beta_i = \tilde{\beta}_i
\]
\[
\forall i \in \{p+1,\hdots, p+q\}: \tilde{\beta}_i = 0
\]
\item<4->
\emph{Special case.} $X_{p+1},\hdots, X_{p+q}$ are pure noise: this is when $\Gamma = 0$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using SNCs to evaluate procedures}
\begin{itemize}
\item Take low-dimensional real data mixed with SNCs (synthetic negative controls), apply inference procedure
\item \emph{Proxy for Type I error:} Rejected SNCs
\item \emph{Proxy for Power:} Rejected original variables
\item If your original data is high-dimensional, apply variable selection to make it low dimensional before conducting this experiment
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{A step-by-step tutorial (in R)}
1. Take the prostate data
\begin{verbatim}
> data(prostate)
> x <- prostate[, 1:8]
> y <- prostate[, 9]
> colnames(x)
[1] "lcavol"  "lweight" "age"     "lbph"    "svi"
    "lcp"     "gleason" "pgg45"  
> dim(x)
[1] 97 8
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{A step-by-step tutorial}
2. Construct 20 synthetic negative controls
\begin{verbatim}
> GAMMA <- matrix(rnorm(8 * 20), 8, 20)
> E <- matrix(rnorm(97 * 20), 97, 20)
> sncs <- as.matrix(x) %*% GAMMA + 2 * E
> sncs <- data.frame(sncs)
> colnames(sncs)
 [1] "X1"  "X2"  "X3"  "X4"  "X5"  "X6" ...
[19] "X19" "X20"
\end{verbatim}
3. Create combined design matrix
\begin{verbatim}
> x2 <- cbind(x, sncs)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{A step-by-step tutorial}
4. Try marginal screening
\begin{verbatim}
> cors <- cor(x2, y)
> cors[order(-abs(cors)), , drop = F]
              [,1]
lcavol   0.7344603
svi      0.5662182
lcp      0.5488132
X6      -0.4591506
X16      0.4482263
lweight  0.4333194
X4      -0.4326898
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{A step-by-step tutorial}
5. Try covariance test
\begin{verbatim}
> library(covTest)
> covTest(lars(as.matrix(x2), y), as.matrix(x2), y)
$results
 Predictor_Number Drop_in_covariance P-value
                1            69.0292  0.0000
                5             1.5390  0.2219
                2             6.8094  0.0020
               11             0.8559  0.4294
\end{verbatim}
(Numbers 1, 5, 2 are original, 11 is a SNC)
\end{frame}

\begin{frame}[fragile]
\frametitle{A step-by-step tutorial}
6. Try debiased lasso (code at http://web.stanford.edu/~montanar/sslasso/)
\begin{verbatim}
> res <- SSLasso(as.matrix(x2), y)
[1] "10% done"
...
[1] "90% done"
> rej <- (res$up < 0) | (res$low > 0)
> names(x2)[rej]
[1] "lcavol"  "lweight" "svi"    
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{A step-by-step tutorial}
7. Try knockoffs
\begin{verbatim}
> library(knockoff)
> knockoff.filter(x2, y)
Call:
knockoff.filter(X = x2, y = y)

Selected variables:
lweight      X7 
      2      15 
\end{verbatim}
\end{frame}

\begin{frame}
\frametitle{Experiments, part 1}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Data & $n$ & $p_1$ & Linear? & Gaussian? & Constant $\sigma^2$?\\ \hline
Personality & 49k & 163 & No & No & No\\ \hline
fMRI & 1750 & 44 & No & OK & No \\ \hline
HIV & 842 & 207 & No & Yes? & OK? \\ \hline
Galaxy & 323 & 4 & No & OK & No \\ \hline
\end{tabular}
\end{center}
\begin{itemize}
\item We add $n/2 - p_1$ synthetic negative controls
\item $X$ is scaled, $\Gamma$ is a gaussian matrix, $Var(E) = Var(X\Gamma)$
\item Multiple trials averaging over the randomness of generating SNCs
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How could this be useful?}
\begin{itemize}
\item Poor performance on benchmarks would tell us where our methods need improvement
\begin{itemize}
\item Failure to control Type I error on benchmarks indicates a need for methods derived under weaker assumptions
\item Overly conservative Type I error control indicates a need for methods which are more adaptive to `easy' cases
\end{itemize}
\item Possible to run a Kaggle-style competition for \emph{inference} rather than prediction
\item Recognizing that different procedures can have differing strengths creates room for a diversity of approaches
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Do we still need to validate on real data?}
\begin{itemize}
\item SNCs can be used to get an idea of worst-case performance on the \emph{hypothesis testing problem} in realistic settings
\item However, how can we tell if the regression framework itself is appropriatefor the real-world problem we are trying to solve?
\item Validation on real data with \emph{scientific} ground truth is still needed
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Closing thoughts}
\begin{quotation}
`` Both the client and the statistician... must base their thinking on
  a recognition that their assumptions will always require review and
  reappraisal...  ''
\end{quotation}
\hfill -- John Tukey
\end{frame}

\begin{frame}
\frametitle{References}
\begin{itemize}
\item Barber, R., and Candes, E. (2014). Controlling the False Discovery Rate via Knockoffs. arXiv Preprint arXiv:1404.5609, 1–27. Retrieved from http://arxiv.org/abs/1404.5609
\item Javanmard, A., and Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15, 2869–2909. Retrieved from http://dl.acm.org/citation.cfm?id=2697057
\item Lockhart, R., Taylor, J., Tibshirani, R. J., and Tibshirani, R. (2014). a Significance Test for the Lasso. Annals of Statistics, 42(2), 413–468. doi:10.1214/13-AOS1175
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Acknowledgements}
Thanks to Will Fithian for useful discussions.
\end{frame}

\end{document}








\[
X = [1 | X_1 | \hdots | X_p] = \begin{pmatrix}x_1^T \\ \vdots \\ x_n^T \end{pmatrix}, \ \ Y = \begin{pmatrix}y_1 \\ \vdots \\ y_n \end{pmatrix}
\]
and
\[
\tilde{X} =  [X | X_{p+1} | \hdots | X_{p+1}] = 
\begin{pmatrix}\tilde{x}_1^T \\ \vdots \\ \tilde{x}_n^T \end{pmatrix}
\]
Let


