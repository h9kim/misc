<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>Introduction</h2>

<p>Suppose \(R_{m \times n}\) is a partially observed matrix:
we only observe \(R_{ij}\) for \((i, j) \in \Omega\).</p>

<p>Here is one way to predict the missing entries.
Consider the following matrix completion problem (Mazumder et al 2010).
\[
\text{minimize}_Z \frac{1}{2}\sum_{(i,j) \in \Omega} (R_{ij} - Z_{ij})^2 + \lambda ||Z||_\star 
\]
After solving this optimization problem, use \(Z_{ij}\) as a prediction of the missing entries \(R_{ij}\).</p>

<p>Consider the solution \(Z\) of this optimization problem.
If \(r = rank(Z)\), then also \(Z = UV^T\), where \(U\), \(V\) are the solutions to
\[
\text{minimize}_{U_{m\times r}, V_{n \times r}} \frac{1}{2} \sum_{(i, j) \in \Omega} (R_{ij} - (UV^T)_{ij})^2 + \frac{\lambda}{2}(||U||^2_F + ||V||^2_F)
\]</p>

<h2>Semi-supervised learning</h2>

<p>Can we interpret matrix completion as a form of semi-supervised learning?
In semi-supervised learning, we have observed covariates \(x_1, &hellip; , x_m \in \mathbb{R}^{n-1}\) and <em>partially</em> observed responses \(y_1, &hellip;, y_{m_0}\) where \(m_0 < m\).
Let \(X_{m \times (n-1)}\) denote the matrix of covariates and \(Y = (y_1, ... , y_m)\) denote the full set of observed and unobserved responses.</p>

<p>The &ldquo;supervised&rdquo; approach would be to fit a model to the fully observed pairs,
\[ Y_i \approx X_i \beta + \beta_0 \]
for \(i = 1,&hellip;, m_0\),
and then predict the unobserved responses as
\(\hat{Y}_i = X_i \beta + \beta_0\) for \(i = m_0 + 1, &hellip; , m\).</p>

<p>Now consider using matrix completion to solve the problem.
Define the matrix \[R_{m \times n} = [X | Y]\]
Here we have observed \(R_{ij}\) for all \(j = 1, &hellip; , n-1\) and for all \(j = n\), \(i = 1, &hellip;, m_0\).
The problem of predicting \(y_{m_0 + 1},&hellip;, y_m\) is equivalent to predicting the missing elements \(R_{m_0 + 1, n}, &hellip;, R_{m, n}\).</p>

<p>Hence we would think of matrix completion as a method for learning a predictive model, which can be used to predict \(Y\) given \(X\).
However, it is not perfectly straightforward to interpret matrix completion as a predictive model like regression.
In regression, the model gives a <em>prediction rule</em> for labelling a new observation \(x_*\).
In matrix completion, if we wanted to predit \(y_*\) for a new observation \(x_*\), we could do so by extending the matrix \(R\) by one row and re-running matrix completion.
However, this process does not seem to be interpretable as a &ldquo;prediction rule.&rdquo;
In the following, we argue that there <em>is</em> a way to interpret matrix completion in terms of a prediction rule: our proposed rule gives different results than re-running matrix completion on an extended matrix \(R\).</p>

</body>

</html>
