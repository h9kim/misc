<p><em>This is a working draft of an article to be posted in the main section.  Comments are welcome and appreciated.  Contents subject to change!</em></p>

<p><em></em><strong>Prerequisites</strong></p>

<p>This post requires some knowledge of Bayesian and Frequentist statistics, as well as probability.  It is intended to explain one of the more advanced concepts in statistical theory--Bayesian non-consistency--to non-statisticians, and although the level required is much less than would be required to read some of the original papers on the topic[1], some considerable background is still required.</p>

<p><strong>The Bayesian dream</strong></p>

<p><strong></strong>Bayesian methods are enjoying a well-deserved growth of popularity in the sciences.  However, most practitioners of Bayesian inference, including most statisticians, see it as a practical tool.  Bayesian inference has many desirable properties for a data analysis procedure: it allows for intuitive treatment of complex statistical models, which include models with random effects, high-dimensional regularization, covariance estimation, outliers.  Many problems which are notoriously difficult to treat from the Frequentist standpoint, such as mixture models and the many-armed bandit problem, can be handled in a relatively straightforward manner using the tools of Bayesian inference.</p>

<p>A more extreme point of view, the flavor of subjective Bayes best exemplified by Jaynes' famous book [2], and also by an sizable contingent of philosophers of science, elevates Bayesian reasoning to <em>the</em> methodology for probabilistic reasoning, in every domain, for every problem.  One merely needs to encode one's beliefs as a prior distribution, and Bayesian inference will yield the optimal decision or inference.</p>

<p>To a philosophical Bayesian, the epistemolgical grounding of most stastics (including "pragmatic Bayes") is abysmal.  The practice of data analysis is either dictated by arbitrary tradion and protocol on the one hand, or consists of users creatively employing a diverse "toolbox" of methods justified by a diverse mixture of theoretical principles like the minimax principle, invariance, asymptotics, maximum likelihood or *gasp* "Bayesian optimality."  The result: a million possible methods exist for any given problem, and a million interpretations exist for any data set, all depending on how one frames the problem.</p>

<p>To me, this ambiguity starts from the very first step where one has to <i>come up with a model for the problem</i>.  A "pragmatic" Bayesian who thinks up a new model for every problem is just as suspect as any Frequentist.  Only by going to the full extreme of the "Bayesian dream" can one escape this ambiguity.  The dream is to never have to <i>choose a model</i> for a problem: instead, start out with one model.  One prior.  <i>For the entire world.</i>  This "world prior" would contain all the entirety of one's own life experience, and the grand total of human knowledge.  But one does not have to write out this prior--that is impossible.  But a true Bayesian must <i>behave</i> (at least approximately) as if they were driven by such a universal prior.  The universal prior <i>generates</i> "sub priors" automatically for any given data analysis problem, at least approximately.  The Bayesian is allowed to change their mind if they realize, "this prior I wrote down is not actually consistent with my unviersal Bayesian reasoning."  All the same, the first step of <i>writing down the model</i> should, ideally be dictated by the universal prior.</p>

<p>Many philosophers of science (e.g. Deborah Mayo) consider subjective Bayesian <i>just as ambiguous</i> as non-Bayesian approaches, since even if you have an unambiguous proecdure for forming <i>personal priors</i>, your priors are still going to differ from mine.  I don't consider this a defect, since my worldview necessarily does differ from yours.  My ultimate goal is to make the best decision for myself.  That said, I do find it possible that this kind of egocentrism is not best suited for a collaborative enterprise like science.</p>

<p>For me, the most far more troublesome objection to the "Bayesian dream" is the question, "How would you go about constructing this prior that represents all of your beliefs?"  Looking in the Bayesian literature, one does not find any convincing examples of any user of Bayesian inference managing to actually encode all (or even a tiny portion) of their beliefs in the form of the prior--in fact, we see alarmingly little thought or justification being put into the construction of the prior.</p>

<p>Nevertheless, I myself remained one of these "hardcore Bayesian", at least from a philosophical point of view, since I started learning about statistics.  My faith in the "Bayesian dream" persisted even after three years in the Ph. D. program in Stanford (a department with a heavy bias towards Frequentism) and even as I personally started doing research in Frequentist methods.  Though I was aware of the Bayesian non-consistency results, I largely dismissed them as mathematical pathologies.  And while we were still a long way from achieving universal inference, I held the optimistic view that improved technology and theory might one day finally make the "Bayesian dream" achievable.  However, one particular example on Wasserman's blog[3] struck me, since it was extremely practical.  Thinking about it some more, I thought of an even simple counterexample, which convinces me of the fundamental impossibility of constructing a universal prior. Perhaps a fellow Bayesian can find a solution to this quagmire, but I am not holding my breath.</p>

<p>
The root of the problem is the extreme degree of ignorance we have about our world, the degree of surprisingness of many true scientific discoveries, and the relative ease with which we accept these surprises.  If we consider this behavior rational, then the subjective Bayesian is obligated to construct a prior which captures this behavior.  Yet, the diversity of possible surprises the model must be able to accommodate makes it practically impossible (if not mathematically impossible) to construct such a prior.
</p>
<p>In the rest of the post, I'll motivate my example, sketch out a few mathematical details (explaining them as best I can to a general audience), then discuss the implications.</p>

<p><b>Introduction: Cancer classification</b></p>

<p>Biology and medicine are currently adapting to the wealth of information we can obtain by using high-throughput assays: technologies which can rapidly read the DNA of an individual, measure the concentration of messenger RNA, metabolites and proteins.  In the early days of this "large-scale" approach to biology, beginning with the Human Genome Project, some optimists might have hoped that such an unprecedented torrent of raw data would allow scientists to quickly "crack the genetic code."  By now, any such optimism has been washed away by the overwhelming complexity and uncertainty of human biology, and of complex diseases like cancer, Alzheimer's, or diabetes.  The data is as valuable as ever, but now we face the prospect that many of the discoveries we can make using this data surpass our current biological understanding.</p>

<p>Now enter the application of <i>machine learning</i> to this large-scale biological data.  Scientists take these massive datasets containing patient outcomes, demographic characteristics, and high-dimensional genetic, neurological, and metabolic data, and analyze them using algorithms like <i>support vector machines</i>, <i>logistic regression</i> and <i>decision trees</i> to learn predictive models to relate key biological variables, "biomarkers", to outcomes of interest.</p>

<p>To give a specific example, take a look at this abstract from the Shipp. et. al. paper on detecting survival rates for cancer patients [4]:</p>

<blockquote>
<p><strong style="font-size: x-small;">Diffuse large B-cell lymphoma (DLBCL), the most common lymphoid malignancy in adults, is curable in less than 50% of patients. Prognostic models based on pre-treatment characteristics, such as the International Prognostic Index (IPI), are currently used to predict outcome in DLBCL. However, clinical outcome models identify neither the molecular basis of clinical heterogeneity, nor specific therapeutic targets. We analyzed the expression of 6,817 genes in diagnostic tumor specimens from DLBCL patients who received cyclophosphamide, adriamycin, vincristine and prednisone (CHOP)-based chemotherapy, and applied a supervised learning prediction method to identify cured versus fatal or refractory disease. The algorithm classified two categories of patients with very different five-year overall survival rates (70% versus 12%). The model also effectively delineated patients within specific IPI risk categories who were likely to be cured or to die of their disease. Genes implicated in DLBCL outcome included some that regulate responses to B-cell&minus;receptor signaling, critical serine/threonine phosphorylation pathways and apoptosis. Our data indicate that supervised learning classification techniques can predict outcome in DLBCL and identify rational targets for intervention.</strong></p>
</blockquote>

<p>The term "supervised learning" refers to any algorithm for learning a predictive model for predicting some outcome <i>Y</i>(could be either categorical or numeric) from covariates or features <i>X</i>.  In this particular paper, the authors used a relatively simple linear model (which they called "weighted voting") for prediction.</p>

<p>A linear model is fairly easy to interpret: it produces a single "score variable" via a weighted average of a number of predictor variables.  Then it predicts the outcome (say "survival" or "no survival") based on a threshold for the score.  Yet, far more advanced machine learning models have been developed, including "deep neural networks" which are winning all of the image recognition and machine translation competitions at the moment. These "deep neural networks" are especially notorious for being difficult to interpret. Along with similarly complicated models, neural networks are often called "black box models": although you can get miraculously accurate answers out of the "box", peering inside won't give you much of a clue as to how it actually works.</p>

<p>Now it is time for the first thought experiment.  Suppose a follow-up paper to the Shipp paper reports dramatically improved prediction for survival rates of lymphoma patients.  The authors of this follow-up paper trained their model on a "training sample" of 500 patients, then used it to predict the five-year outcome of chemotherapy patients, on a "test sample" of 1000 patients.  It correctly predicts the outcome ("survival" vs "no survival") on 990 of the 1000 patients.</p>
<p>Question 1: what is your opinion on the predictive accuracy of this model on the population of chemotherapy patients?  Suppose that publication bias is not an issue (the authors of this paper designed the study in advance and committed to publishing) and suppose that the test sample of 1000 patients is "representative" of the entire population of chemotherapy patients.</p>

<p>Question 2: does your judgment depend on the <i>complexity</i> of the model they used?  What if the authors used an extremely complex and counterintuitive model, and cannot even offer any justification or explanation for why it works? (Nevertheless, their peers have independently confirmed the predictive accuracy of the model.)</p>

<p><b>A Frequentist approach</b></p>

<p>The Frequentist answer to the thought experiment is as follows.  The accuracy of the model is a probability <i>p</i> which we wish to estimate.  The number of successes on the 1000 test patients is <i>Binomial(p, 1000)</i>.  Based on the data, one can construct a confidence interal: say, we are 99% confident that the accuracy is above 83%.  <i>What does 99% confident mean?</i>  I won't try to explain, but simply say that in this particular situation, "I'm pretty sure" that the accuracy of the model is above 83%.</p>

<p><b>A Bayesian approach</b></p>

<p>The Bayesian interjects, "Hah! You can't explain what your confidence interval actually means!"  He puts a uniform prior on the probability <i>p</i>.  The posterior distribution of <i>p</i>, conditional on the data, is <i>Beta(1000, 2)</i>. This gives a 99% <i>credible interval</i> that <i>p</i> is in [0.996, 0.9998].  You can actually interpret the interval in probabilistic terms, and it gives a much tighter interval as well.  Seems like a Bayesian victory...?</p>

<p><b>A subjective Bayesian approach</b></p>

<p>As I have argued before, a Bayesian approach which comes up with a model <i>after hearing about the problem</i> is bound to suffer from the same inconsistency and arbitariness as any non-Bayesian approach.  You might assume a uniform distribution for <i>p</i> in this problem... but yet another paper comes along with a similar prediciton model?  You would need a join distribution for the current model and the new model.  What if a theory comes along that could help explain the success of the current method?  The parameter <i>p</i> might take a new meaning in this context.</p>

<p>So as a subjective Bayesian, I argue that slapping a uniform prior on the accuracy is the wrong approach.  But I'll stop short of actually constructing a Bayesian model of the entire world: let's say we want to restrict our attention to this particular issue of cancer prediction.  We want to model the dynamics behind cancer and cancer treatment in humans.  Needless to say, the model is still ridiculously complicated.  However, I don't think it's out of reach of the efforts of a well-funded, large collaborative effort of scientists.</p>

<p>Roughly speaking, the model can be divided into a <i>distribution over theories of human biology</i>, and conditional on the theory of biology, a course-grained model of an individual patient.  The model would not include every cell, every molecule, etc., but it would contain many latent variables in addition to the variables measured in any particular cancer study.  Let's call the variables actually measured in the study, <i>X</i>, and also the survival outcome, <i>Y</i>.</p>

<p>Now here is the correct way to answer the thought experiment.  Take a look at the X's and Y's of the patients in the training and test set.  Update your probabilistic model of human biology based on the data.  Then take a look at the actual form of the classifier: it's a function <i>f()</i> mapping X's to Y's.  The accuracy of the classsifer is no longer parameter: it's a quantity <b>Pr</b>[f(X) = Y] <i>which has a distribution under your posterior</i>.  That is, for any given "theory of human biology", <b>Pr</b>[f(X) = Y] has a fixed value: now, over the distribution of possible theories of human biology (based on the data of the current study as well as all previous studies and your own beliefs) <b>Pr</b>[f(X) = Y] has a distribution, and therefore, an average.  But what will this posterior give you?  Will you get something similar to the interval [0.996, 0.9998] you got from the "practical Bayes" approach?</p>

<p>I doubt it.  If the classification rule <i>f</i> is sufficiently complicated, there is no way you could have anticipated a function of this form having a high correlation to <i>Y</i> given the prior you constructed.  On the other hand, you could have tried to make your priors sufficiently flexible in advance to handle this surprise, in addition to a billion billion other possible surprises (that will actually never occur.)  Yet this would have the effect of watering down your prior too much to be able to draw any conclusions.  And to come up with a prior that strikes the right balance between flexibility and power, is a task, which I am convinced cannot be solved in this century or the next.</p>

<p>I don't expect this example to convince you: it is difficult to reason about such a model of "all possible theories of human biology."  The example which convinces me is a much more conservative Bayesian approach to this problem: one which does not go as far as to "incorporate all prior beliefs" but also does not strike me as unacceptably <i>ad hoc</i>.</p>

<p><b>A nonparametric Bayesian approach</b></p>

<p>Let us accept the fact that a full Bayesian model of biology is either out of reach, or acheivable but undesirably brittle.  We still strive towards the ideal of the "Bayesian dream," and at the very least we should try to include the X's and the Y's in the model, not merely abstracting the whole experiment as a Binomial trial.  Hence we need a prior over <i>joint distributions of (X, Y)</i>.  And yes, I do mean a prior distribution <i>over</i> probability distributions: we are saying that (X, Y) has some unknown joint distribution, which we treat as being drawn at random from a large collection of distributions.  This is therefore a <i>non-parametric Bayes</i> approach: the term non-parametric means that the number of the parameters in the model is not finite.</p>

<p>Since in this case Y is a binary outcome, a joint distribution can be decomposed as a <i>marginal distribution</i> over X, and a function <i>g(x)</i> giving the conditional probability that Y=1 given X=x.  The marginal distribution is not so interesting or important for us, since it simple reflects the composition of the population of patients.  For the purpose of this example, let us say that the marginal is known (e.g., a finite distribution over the population of US cancer patients).  What we want to know is the probability of patient survival, and this is given by the function g(X) for the particular patient's X.  Hence, we will mainly deal with constructing a prior over g(X).</p>

<p>To construct a prior, we need to think of intuitive properties of the survival probability function g(x).  If x is similar to x', then we expect the survival probabilities to be similar.  Hence the prior on g(x) should be over <i>random, smooth functions</i>.  But we need to choose the smoothness so that the prior does not consist of almost-constant functions.  Suppose for now that we choose particular smoothness (however we measure it) and choose our prior to to be <i>uniform</i> over functions of that smoothness.  We could go further and put a prior on the smoothness hyperparameter, but for now we won't.</p>

<p>Now, accepting the need to be at least a littel bit pragmatic, we want to think about how whatever prior we choose would allow use to answer some simple though experiments.  Take the following thought experiment: we simply want to estimate the expected value of Y, E[Y].  Hence, we draw 100 patients independently with replacement from the population and record their outcomes: suppose the sum is 80 out of 100.  The Frequentist (and prgamatic Bayesian) would end up concluding that with high probability/confidence/whatever, the expected value of Y is around 0.8.  But what would our nonparametric model say?  We would draw a random function g(x) conditional on our particular observations: we get a quantity E[g(X)] for each instantiation of g(x): the distribution of E[g(X)]'s over the posterior allows us to make credible intervals for E[Y].</p>

<p>But what do we end up getting?  Either one of two things happens.  Either you choose too little smoothness, and E[g(X)] ends up concentrating at around 0.5, <i>no matter what data you put into the model</i>.  This is the phenomenon of Bayesian non-consistency, and a detailed explanation can be found in several of the listed references: but to put it briefly, sampling at a few isolated points gives you too little information on the rest of the function.  This example is not as pathological as the ones used in the literature: if you sample infinitely many points, you will eventually get the posterior to concentrate around the true value of E[Y], but all the same, the convergence is ridiculously slow.  Alternatively, use a super-high smoothness, and the posterior of E[g(X)] has a nice interval around the sample value just like in the Binomial example.  But now if you look at your posterior draws of g(x), you'll notice the functions are basically constants.  Putting a prior on smoothness doesn't change things: the posterior on smoothness doesn't change, since you don't actually have enough data to determine the smoothness of the function.  The posterior average of E[g(X)] is no longer always 0.5: it gets a little bit affected by the data, since within the 10% mass of the posterior corresponding to the smooth prior, the average of E[g(X)] is responding to the data.  But you are still almost as slow as before in converging to the truth. </p>

<p>At the time that I started thinking about the above "uniform sampling" example, I was stil convinced of a Bayesian resolution.  Obviously, using a uniform prior over smooth functions is too naive: you can tell by seeing that the <i>prior distribution</i> over E[g(X)] is already highly concentrated around 0.5.  How about a hierarchical model, where first we draw a parameter <i>p</i> from the uniform distribution, and then draw <i>g(x)</i> from the uniform distribution over smooth functions <i>with mean value equal to p</i>?  This gets you non-constant g(x) in the posterior, while your posteriors of E[g(X)] converge to the truth as quickly as in the Binomial example.</p>

<p>But then I thought, what about more complicated problems than computing E[Y]?  What if you have to compute the expectation of Y conditional on some complicated function of X taking on a certain value: i.e. E[Y|f(X) = 1]?  In the frequentist world, you can easily compute E[Y|f(X)=1] by rejection sampling: get a sample of individuals, average the Y's of the individuals whose X's satisfy f(X) = 1.  But how could you formulate a prior that has the same property?  For a finite collection of functions f, {f1,...,f100}, say, you might be able to construct a prior for g(x) so that the posterior for E[g(X)|fi = 1] converges to the truth for every i in {1,..,100}.  I don't know how to do so, but perhaps you know.  But the frequentist intervals work for every function f!  Can you construct a prior which can do the same?</p>

<p>I am happy to argue that a true Bayesian would not need consistency for every possible f in the mathematical universe.  It is cool that frequentist inference works for such a general collection: but it may well be unnecessary for the world we live in.  Yet, the set of f's which could potentially be relevant could well be so massive as to render impossible the construction of a nonparametric prior which can be consistent for all quantities relating to those f's, such as E[g(X)|f], or in the cancer example, E[Y = f(X)].  That is the case even if we stick to functions f which are may conceivably arise in the natural world according to conventional science.  And yet the Bayesian dream makes the far stronger demand that our prior match the characteristics of rational thought, which can come to accommodate uncountably many surprising potential truths, supposing there is enough evidence to establish any particular truth.</p>

<p><b>Discussion</b></p>

To be updated later... perhaps responding to some of your comments!

<p>&nbsp;</p>
<p>[1] Diaconis and Freedman, "On the Consistency of Bayes Estimates"</p>
<p>[2] ET Jaynes, <em>Probability: the Logic of Science</em></p>
<p>[3] https://normaldeviate.wordpress.com/2012/08/28/robins-and-wasserman-respond-to-a-nobel-prize-winner/</p>
<p>[4] Shipp et al. "Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning." <i>Nature</i></p>
