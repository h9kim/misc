{
 "metadata": {
  "name": "",
  "signature": "sha256:728862046de19748819efca7ea60557038cb269cdcfc3de092cb0b181fee1b61"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# False discovery rate control with empirical null distributions\n",
      "\n",
      "<br/>\n",
      "<br/>\n",
      "<br/>\n",
      "<br/>\n",
      "\n",
      "## Presentation by Charles Zheng\n",
      "\n",
      "\n",
      "<br/><br/><br/><br/>\n",
      "\n",
      "Reference: A Schwartzmann, R Dougherty, *et al*.  `Empirical null and false discovery rate analysis in neuroimaging'. *NeuroImage* 2008"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Are you using the right null distribution??!\n",
      "\n",
      "<img src = 'images/paper1.png'/> \n",
      "\n",
      "Dashed: theoretical null (N(0,1)).  Solid: empirical null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src = 'images/paper2.png'/>\n",
      "\n",
      "Dashed: theoretical null $\\chi^2$.  Solid: empirical null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Estimating the 'null'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      " * Assume that the actual null distribution is a shifted or scaled version of the theoretical null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      " * Assume that the test statistics in most (90% + ) of the voxels are drawn from the null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      " * Estimate the null distribution from the 'bulk' of the empirical distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Example 1\n",
      " * Simulate 10,000 voxels, voxels[0:100] are non-null and voxels[100:10000] are null \n",
      " * Non-null voxels are drawn from $N(-.3, 5)$\n",
      " * Null voxels are drawn from $N(0.2, 1.2)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as spst\n",
      "mu_null = 0.2\n",
      "sigma2_null = 1.2\n",
      "mu_alt = -0.3\n",
      "sigma2_alt = 5\n",
      "nvoxels = 10000\n",
      "n_alt = 100 # number of non-null voxels\n",
      "voxels = np.zeros(nvoxels)\n",
      "# sample the non-null voxels\n",
      "voxels[:n_alt] = npr.normal(mu_alt, np.sqrt(sigma2_alt), n_alt) \n",
      " # sample the null voxels\n",
      "voxels[n_alt:] = npr.normal(mu_null, np.sqrt(sigma2_null), nvoxels-n_alt)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# plot the test statistics\n",
      "nbins = 30\n",
      "hist = plt.hist(voxels, nbins)\n",
      "binwidth = hist[1][1] - hist[1][0]\n",
      "plt.title('Histogram vs theoretical null', fontsize = 20)\n",
      "# compare with the theoretical null N(0,1)\n",
      "xs = np.arange(-8, 8, 0.1)\n",
      "y = spst.norm.pdf(xs)\n",
      "plt.plot(xs, y * nvoxels * binwidth)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'est_null'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Estimating the null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      " * Use only the data from the 'bulk' (the 1st to 3rd quantiles)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_voxels = sorted(voxels)\n",
      "q_1 = int(.25 * nvoxels)\n",
      "q_3 = int(.75 * nvoxels)\n",
      "bulk_voxels = np.array(sorted_voxels[q_1:q_3])\n",
      "plt.hist(bulk_voxels, bins = 30)\n",
      "plt.title('The bulk of the distribution', fontsize = 20)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      " * We assume that the bulk contains mostly samples from the null distribution\n",
      " * Since this is a simulation, we can actually check that assumption"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_alt = sum(np.logical_and(voxels[:n_alt] >= min(bulk_voxels), voxels[:n_alt] <= max(bulk_voxels)))\n",
      "count_null =  sum(np.logical_and(voxels[n_alt:] >= min(bulk_voxels), voxels[n_alt:] <= max(bulk_voxels)))\n",
      "print('Number of null voxels in bulk: ' + str(count_null))\n",
      "print('Number of non-null voxels in bulk: ' + str(count_alt))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Fitting the null distribution\n",
      "\n",
      "The null density is given by\n",
      "\n",
      "$$\n",
      "f_0(z) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(\\frac{-(z-\\mu)^2}{2\\sigma^2}\\right)\n",
      "$$\n",
      "\n",
      "which can be written as\n",
      "\n",
      "$$\n",
      "\\log f_0(z) = b_0 + b_1 z + b_2 z^2\n",
      "$$\n",
      "\n",
      "where\n",
      "\n",
      "$$\n",
      "b_1 = \\frac{\\mu}{\\sigma^2}\n",
      "$$\n",
      "\n",
      "$$\n",
      "b_2 = -\\frac{1}{2\\sigma^2}\n",
      "$$\n",
      "\n",
      "Therefore $\\mu, \\sigma^2$ can be estimated using Poisson or OLS regression using the log-counts (to be demonstrated)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Demonstration using OLS (for simplicity)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1) Form bin midpoints and counts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(min(bulk_voxels), max(bulk_voxels))\n",
      "range_bins = max(bulk_voxels)-min(bulk_voxels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nbins = 20\n",
      "\n",
      "# the left edges of the bins\n",
      "bin_lefts = min(bulk_voxels) + np.arange(0, 1, 1.0/nbins) * range_bins\n",
      "# the right edges of the bins\n",
      "bin_rights = min(bulk_voxels) + np.arange(1.0/nbins, 1 + 1e-14, 1.0/nbins) * range_bins\n",
      "# the midpoints\n",
      "bin_midpoints = (bin_lefts + bin_rights)/2.0\n",
      "# the counts\n",
      "bin_counts = [sum((bulk_voxels >= l) * (bulk_voxels < r)) for (l, r) in zip(bin_lefts, bin_rights)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "float_formatter = lambda x: \"%5.2f\" %x\n",
      "np.set_printoptions(formatter={'float_kind': float_formatter})\n",
      "np.vstack([bin_lefts, bin_rights, bin_counts]).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Regress log(counts) on midpoints"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(bin_midpoints, np.log(bin_counts))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\log(\\text{bin centered at } z) = b_0 + b_1 z + b_2 z^2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# response is log(counts)\n",
      "yvec = np.log(bin_counts)\n",
      "# design matrix is (1, midpoints, midpoints^2)\n",
      "xmat = np.vstack([np.ones(nbins), bin_midpoints, bin_midpoints**2]).T\n",
      "regression_result = np.linalg.lstsq(xmat, yvec)\n",
      "coeffs = regression_result[0]\n",
      "yhat = np.squeeze(np.dot(xmat, coeffs))\n",
      "plt.scatter(bin_midpoints, np.log(bin_counts))\n",
      "plt.plot(bin_midpoints, yhat)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) Convert regression coefficients to parameter estimates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model:\n",
      "$$\\log(\\text{bin centered at } z) = b_0 + b_1 z + b_2 z^2$$\n",
      "\n",
      "Truth:\n",
      "$$\\log(\\text{nulls in bin centered at } z) \\approx \\text{const} + \\frac{\\mu}{\\sigma^2} z - \\frac{1}{2\\sigma^2} z^2$$\n",
      "\n",
      "Thus:\n",
      "$$ \\sigma^2 \\approx -\\frac{1}{2b_2}$$\n",
      "$$ \\mu \\approx b_1 \\sigma^2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b0 = coeffs[0]\n",
      "b1 = coeffs[1]\n",
      "b2 = coeffs[2]\n",
      "sigma2_hat = -1.0/(2*b2)\n",
      "mu_hat = b1 * sigma2_hat\n",
      "print('Estimate of mu: ' + str(mu_hat))\n",
      "print('Estimate of sigma^2: ' + str(sigma2_hat))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4) Since this is a simulation, compare to ground truth"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# actual null\n",
      "xs = np.arange(-8, 8, 0.1)\n",
      "y0 = spst.norm.pdf(xs, loc = mu_null, scale = np.sqrt(sigma2_null))\n",
      "plt.plot(xs, y0 * nvoxels * binwidth)\n",
      "# estimated null\n",
      "y1 = spst.norm.pdf(xs, loc = mu_hat, scale = np.sqrt(sigma2_hat))\n",
      "plt.plot(xs, y1 * nvoxels * binwidth)\n",
      "print(\"Blue: true null, Green: estimated null\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Controlling the FDR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Using the misspecified theoretical Null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plot of the nulls and alternative test statistics ordered by **theoretical null** p-value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alt_indicator = np.array([i <= n_alt for i in range(nvoxels)])\n",
      "pvalues_theory = spst.norm.cdf(voxels, loc = 0.0, scale = 1.0)\n",
      "temp = zip(list(pvalues_theory), list(alt_indicator))\n",
      "ordered_pvalues_indicators_theory = sorted(temp)\n",
      "ordered_pvalues_theory,ordered_indicators_theory = zip(*ordered_pvalues_indicators_theory)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_theory)), np.array(ordered_indicators_theory))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plot of the FDP (false positives / positives) by rejection threshold"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "true_positives_theory = np.cumsum(np.array(ordered_indicators_theory))\n",
      "fdp_theory = 1 - true_positives_theory/(np.array(range(nvoxels), dtype=float)+1)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_theory)), fdp_theory)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The BH \"estimate\" of FDP is obtained as follows:\n",
      "$$\n",
      "FDP(p_{(i)}) = p_{(i)} \\frac{n}{i}\n",
      "$$\n",
      "Compare the BH estimate (red) to the true FDP (blue)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bh_est_theory = np.array(ordered_pvalues_theory) * nvoxels/(np.array(range(nvoxels)) + 1)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_theory)), fdp_theory)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_theory)), bh_est_theory, c = 'r')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Using the correct but unknown null (from simulation)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plot of the nulls and alternative test statistics ordered by **true null** p-value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alt_indicator = np.array([i <= n_alt for i in range(nvoxels)])\n",
      "pvalues_true = spst.norm.cdf(voxels, loc = mu_null, scale = np.sqrt(sigma2_null))\n",
      "temp = zip(list(pvalues_true), list(alt_indicator))\n",
      "ordered_pvalues_indicators_true = sorted(temp)\n",
      "ordered_pvalues_true,ordered_indicators_true = zip(*ordered_pvalues_indicators_true)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_true)), np.array(ordered_indicators_true))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plot of the FDP (false positives / positives) by rejection threshold"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The BH \"estimate\" of FDP is obtained as follows:\n",
      "$$\n",
      "FDP(p_{(i)}) = p_{(i)} \\frac{n_0}{i}\n",
      "$$\n",
      "where $n_0$ is the number of null hypotheses.\n",
      "Compare the BH estimate (red) to the true FDP (blue)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "true_positives_true = np.cumsum(np.array(ordered_indicators_true))\n",
      "fdp_true = 1 - true_positives_true/(np.array(range(nvoxels), dtype=float)+1)\n",
      "bh_est_true = np.array(ordered_pvalues_true) * (nvoxels-n_alt)/(np.array(range(nvoxels)) + 1)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_true)), fdp_true)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_true)), bh_est_true, c = 'r')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id = 'emp_null'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Using the **empirical** null"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plot of the nulls and alternative test statistics ordered by **empirical null** p-value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alt_indicator = np.array([i <= n_alt for i in range(nvoxels)])\n",
      "pvalues_emp = spst.norm.cdf(voxels, loc = mu_hat, scale = np.sqrt(sigma2_hat))\n",
      "temp = zip(list(pvalues_emp), list(alt_indicator))\n",
      "ordered_pvalues_indicators_emp = sorted(temp)\n",
      "ordered_pvalues_emp,ordered_indicators_emp = zip(*ordered_pvalues_indicators_emp)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_emp)), np.array(ordered_indicators_emp))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The BH \"estimate\" of FDP is obtained as follows:\n",
      "$$\n",
      "FDP(p_{(i)}) = p_{(i)} \\frac{\\hat{n}_0}{i}\n",
      "$$\n",
      "where $\\hat{n_0}$ is the estimated number of nulls.  Here take $\\hat{n}_0 = n$.\n",
      "Compare the BH estimate (red) to the true FDP (blue)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "true_positives_emp = np.cumsum(np.array(ordered_indicators_emp))\n",
      "fdp_emp = 1 - true_positives_emp/(np.array(range(nvoxels), dtype=float)+1)\n",
      "bh_est_emp = np.array(ordered_pvalues_emp) * nvoxels/(np.array(range(nvoxels)) + 1)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_emp)), fdp_emp)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_emp)), bh_est_emp, c = 'r')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also estimate $\\hat{n}_0$ by *extrapolating* our regression\n",
      "$$\\log(\\text{bin centered at } z) = b_0 + b_1 z + b_2 z^2$$\n",
      "Thus\n",
      "$$n_0 \\approx \\sum_{\\text{bin centers}}\\exp(b_0 + b_1 z + b_2 z^2)$$\n",
      "where the sum is taken over *extended* bins from $-\\infty$ to $\\infty$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bin_width = bin_rights[0]-bin_lefts[0]\n",
      "extended_bin_midpoints = bin_midpoints[0] + bin_width * np.arange(-100, 100, 1.0)\n",
      "n0_hat = sum(np.exp((b0 + b1 * extended_bin_midpoints + b2 * extended_bin_midpoints**2)))\n",
      "n0_hat, nvoxels-n_alt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "true_positives_emp = np.cumsum(np.array(ordered_indicators_emp))\n",
      "fdp_emp = 1 - true_positives_emp/(np.array(range(nvoxels), dtype=float)+1)\n",
      "bh_est_emp = np.array(ordered_pvalues_emp) * n0_hat/(np.array(range(nvoxels)) + 1)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_emp)), fdp_emp)\n",
      "plt.scatter(np.log(np.array(ordered_pvalues_emp)), bh_est_emp, c = 'r')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bonus 1. Using Poisson regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The intuition behind Poisson regression is as follows.\n",
      "\n",
      "The density at $z$ is $\\exp\\left(const - \\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$\n",
      "\n",
      "Rewrite $\\exp\\left(const - \\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$ as $\\frac{1}{n_0}\\exp(b_0 + b_1 z + b_2 z^2)$\n",
      "\n",
      "Now imagine drawing the null test statistics one at a time.\n",
      "\n",
      "Each time a new test statistic is generated, the probability it falls into a particular bin with center $z$ is $\\frac{1}{n_0}\\exp(b_0 + b_1 z + b_2 z^2)$.\n",
      "\n",
      "Adding up these $n_0$ independent binary variables, the total number of statistics in a bin with center $z$ is a Poisson distribution with parameter $\\frac{n_0}{n_0}\\exp(b_0 + b_1 z + b_2 z^2)$.\n",
      "\n",
      "Recall the probability mass function function of Poisson with parameter $\\lambda$ is\n",
      "\n",
      "$$Pr[Y = y] = \\frac{e^{-\\lambda} \\lambda^y}{y!}$$\n",
      "\n",
      "Inside the log-likelihood, the $y!$ term drops out because it does not depend on $\\lambda$.\n",
      "The contribution to the log-likehood is\n",
      "$$\n",
      "-\\lambda + y \\log(\\lambda)\n",
      "$$\n",
      "\n",
      "Now recall that the loss is the negative log-likelihood, and that in our case\n",
      "$$\n",
      "y = \\text{(count in) bin center at }z\n",
      "$$\n",
      "\n",
      "$$\n",
      "\\lambda = \\exp(b_0 + b_1 z + b_2 z^2)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Poisson regression minimizes the following\n",
      "$$\n",
      "loss = \\sum \\exp(b_0 + b_1 z + b_2 z^2) - (\\text{bin centered at }z) (b_0 + b_1 z + b_2 z^2)  \n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def poisson_loss(coeffs):\n",
      "    yh = np.squeeze(np.dot(xmat, coeffs))\n",
      "    return sum(np.exp(yh)) - sum(bin_counts * yh)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.optimize as spo\n",
      "res = spo.fmin(poisson_loss, coeffs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b0_p = res[0]\n",
      "b1_p = res[1]\n",
      "b2_p = res[2]\n",
      "sigma2_hat_p = -1.0/(2*b2_p)\n",
      "mu_hat_p = b1_p * sigma2_hat\n",
      "print('Estimate of mu (poisson): ' + str(mu_hat_p))\n",
      "print('Estimate of sigma^2 (poisson): ' + str(sigma2_hat_p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare to OLS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Estimate of mu (OLS): ' + str(mu_hat))\n",
      "print('Estimate of sigma^2 (OLS): ' + str(sigma2_hat))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('mu (true): ' + str(mu_null))\n",
      "print('sigma^2 (true): ' + str(sigma2_null))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the FDP (go back to the earlier section)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu_hat_ols = mu_hat\n",
      "sigma2_hat_ols = sigma2_hat\n",
      "coeffs_ols = coeffs\n",
      "mu_hat = mu_hat_p\n",
      "sigma2_hat = sigma2_hat_p\n",
      "b0 = b0_p\n",
      "b1 = b1_p\n",
      "b2 = b2_p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Estimate the empirical null FDP](#emp_null)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br/><br/><br/><br/><br/><br/><br/><br/><br/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bonus 2. T-statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of t-statistics rather than z-statistics, the statistics are converted into z-statistics via quantile transform"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = 30 # the degrees of freedom\n",
      "voxels_z = voxels\n",
      "voxels_t = voxels/np.sqrt(spst.chi2.rvs(df = df, size = nvoxels)/df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdfs = spst.t.cdf(voxels_t, df = 30)\n",
      "voxels_q = spst.norm.ppf(cdfs)\n",
      "plt.scatter(voxels_t, voxels_q)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Redo the eariler analysis using the transformed statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "voxels = voxels_q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Estimate the empirical null](#est_null)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}